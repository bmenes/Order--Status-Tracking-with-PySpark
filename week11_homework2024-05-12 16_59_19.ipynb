{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e62b29-d595-4705-af74-67df1595edf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958d3848-92cc-4af9-8e12-f61361a7780c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.load(\"/FileStore/tables/part_00000_4fb2c33a_49da_4456_991f_f96fb8ec06d9_c000.csv\",format=\"csv\",sep=\",\",header=\"true\",inferSchema=\"true\")\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9b464a-dff6-4c9c-971e-c8889de68bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ORDER_ID: integer (nullable = true)\n |-- SUBSCRIBER_ID: integer (nullable = true)\n |-- STATUS_DATE: integer (nullable = true)\n |-- STATUS_TIME: integer (nullable = true)\n |-- STATUS: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebba4c31-6aae-4ecb-8cd4-a4ff7d7a713f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074818fa-9e19-41b6-9ebb-f1cb03930df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+-------------------+-------------------+---------+\n|ORDER_ID|SUBSCRIBER_ID|   STATUS|         START_DATE|           END_DATE| DURATION|\n+--------+-------------+---------+-------------------+-------------------+---------+\n|  100001|       200574| ASSIGNED|2023-02-22 09:53:06|               null|      0.0|\n|  100002|       200121|     POOL|2023-02-22 09:37:12|               null|      0.0|\n|  100003|       200432| ASSIGNED|2023-02-22 09:40:01|               null|      0.0|\n|  100004|       200234|COMPLETED|2023-02-22 09:37:58|2023-02-22 10:44:26| 1.107778|\n|  100005|       200546|     POOL|2023-02-22 09:47:27|               null|      0.0|\n|  100006|       200369| ASSIGNED|2023-02-22 10:54:13|               null|      0.0|\n|  100007|       200486| ASSIGNED|2023-02-22 10:54:48|               null|      0.0|\n|  100008|       200190| ASSIGNED|2023-02-22 09:43:31|               null|      0.0|\n|  100009|       200058|COMPLETED|2023-02-22 09:49:51|2023-02-22 19:55:48|10.099167|\n|  100010|       200253|COMPLETED|2023-02-22 09:43:01|2023-02-22 12:07:25| 2.406667|\n+--------+-------------+---------+-------------------+-------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ORDER_ID AS ORDER_ID,\n",
    "        SUBSCRIBER_ID AS SUBSCRIBER_ID,\n",
    "        first(STATUS) AS STATUS,\n",
    "        max(\n",
    "            CASE\n",
    "            WHEN STATUS = 'CREATED' OR STATUS = 'POOL' THEN \n",
    "            TO_TIMESTAMP(CONCAT(STATUS_DATE,lpad(STATUS_TIME, 6, '0')),'yyyyMMddHHmmss')\n",
    "            else NULL\n",
    "            END \n",
    "        ) as START_DATE,\n",
    "         max(\n",
    "            CASE\n",
    "            WHEN STATUS = 'COMPLETED' or STATUS=\"CANCELLED\" THEN \n",
    "            TO_TIMESTAMP(CONCAT(STATUS_DATE,lpad(STATUS_TIME, 6, '0')),'yyyyMMddHHmmss')\n",
    "            else NULL\n",
    "            END \n",
    "        ) as END_DATE,\n",
    "        \n",
    "            CASE when END_DATE IS NOT NULL THEN  round((unix_timestamp(End_Date) - unix_timestamp(Start_Date)) / 3600 ,6)\n",
    "            ELSE 0.0\n",
    "            END  AS DURATION\n",
    "       \n",
    "        from orders\n",
    "        GROUP BY ORDER_ID,SUBSCRIBER_ID\n",
    "        ORDER BY \n",
    "        ORDER_ID ASC, min(to_timestamp(concat(STATUS_DATE, lpad(STATUS_TIME, 6, '0')), 'yyyyMMddHHmmss')) DESC\n",
    "        limit 10\n",
    "  \n",
    "\"\"\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "week11_homework2024-05-12 16:59:19",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
